{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf826bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import analytics_core_V04 as ac\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f5a0678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import combined_functions as cf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a4e0e",
   "metadata": {},
   "source": [
    "# Figure 2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f4552193",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_used = r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol'\n",
    "\n",
    "experiment_dict = [\n",
    "    {'instrument': '0.25 ng', 'method' : 'In sol', 'file_tags': ['A1', 'A2', 'A3', 'A4'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\InSol\\0p25ng\\report.parquet'},\n",
    "    {'instrument': '1 ng', 'method' : 'In sol', 'file_tags': ['B1', 'B2', 'B3', 'B4'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\InSol\\1ng\\report.parquet'},\n",
    "    {'instrument': '5 ng', 'method' : 'In sol', 'file_tags': ['C1', 'C2', 'C3', 'C4'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\InSol\\5ng\\report.parquet'},\n",
    "    {'instrument': '20 ng', 'method' : 'In sol', 'file_tags': ['D1', 'D2', 'D3', 'D4'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\InSol\\20ng\\report.parquet'},\n",
    "    {'instrument': '50 ng', 'method' : 'In sol', 'file_tags': ['E1', 'E2', 'E3', 'E4'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\InSol\\50ng\\report.parquet'},\n",
    "    {'instrument': '200 ng', 'method' : 'In sol', 'file_tags': ['F1', 'F2', 'F3', 'F4'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\InSol\\200ng\\report.parquet'},\n",
    "    {'instrument': '500 ng', 'method' : 'In sol', 'file_tags': ['G1', 'G2', 'G3', 'G4'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\InSol\\500ng\\report.parquet'},\n",
    "    {'instrument': '1000 ng', 'method' : 'In sol', 'file_tags': ['H1', 'H2', 'H3', 'H4'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\InSol\\1000ng\\report.parquet'},\n",
    "    {'instrument': '0.25 ng', 'method' : 'PAC', 'file_tags': ['A9', 'A10', 'A11', 'A12'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\PAC\\0p25ng\\report.parquet'},\n",
    "    {'instrument': '1 ng', 'method' : 'PAC', 'file_tags': ['B9', 'B10', 'B11', 'B12'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\PAC\\1ng\\report.parquet'},\n",
    "    {'instrument': '5 ng', 'method' : 'PAC', 'file_tags': ['C9', 'C10', 'C11', 'C12'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\PAC\\5ng\\report.parquet'},\n",
    "    {'instrument': '20 ng', 'method' : 'PAC', 'file_tags': ['D9', 'D10', 'D11', 'D12'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\PAC\\20ng\\report.parquet'},\n",
    "    {'instrument': '50 ng', 'method' : 'PAC', 'file_tags': ['E9', 'E10', 'E11', 'E12'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\PAC\\50ng\\report.parquet'},\n",
    "    {'instrument': '200 ng', 'method' : 'PAC', 'file_tags': ['F9', 'F10', 'F11', 'F12'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\PAC\\200ng\\report.parquet'},\n",
    "    {'instrument': '500 ng', 'method' : 'PAC', 'file_tags': ['G9', 'G10', 'G11', 'G12'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\PAC\\500ng\\report.parquet'},\n",
    "    {'instrument': '1000 ng', 'method' : 'PAC', 'file_tags': ['H9', 'H10', 'H11', 'H12'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\PAC\\1000ng\\report.parquet'},\n",
    "    {'instrument': '0.25 ng', 'method' : 'SPEC', 'file_tags': ['A5', 'A6', 'A7', 'A8'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\SPEC\\0p25ng\\report.parquet'},\n",
    "    {'instrument': '1 ng', 'method' : 'SPEC', 'file_tags': ['B5', 'B6', 'B7', 'B8'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\SPEC\\1ng\\report.parquet'},\n",
    "    {'instrument': '5 ng', 'method' : 'SPEC', 'file_tags': ['C5', 'C6', 'C7', 'C8'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\SPEC\\5ng\\report.parquet'},\n",
    "    {'instrument': '20 ng', 'method' : 'SPEC', 'file_tags': ['D5', 'D6', 'D7', 'D8'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\SPEC\\20ng\\report.parquet'},\n",
    "    {'instrument': '50 ng', 'method' : 'SPEC', 'file_tags': ['E5', 'E6', 'E7', 'E8'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\SPEC\\50ng\\report.parquet'},\n",
    "    {'instrument': '200 ng', 'method' : 'SPEC', 'file_tags': ['F5', 'F6', 'F7', 'F8'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\SPEC\\200ng\\report.parquet'},\n",
    "    {'instrument': '500 ng', 'method' : 'SPEC', 'file_tags': ['G5', 'G6', 'G7', 'G8'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\SPEC\\500ng\\report.parquet'},\n",
    "    {'instrument': '1000 ng', 'method' : 'SPEC', 'file_tags': ['H5', 'H6', 'H7', 'H8'], 'path': r'Z:\\Tim_SPEC\\figure2\\K562_dilution_series_SPEC_vs_InSol\\SPEC\\1000ng\\report.parquet'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97551742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "d:\\Projects\\SPEC\\scripts_for_figs\\combined_functions.py:749: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group experiments by file\n",
    "experiments_by_file = defaultdict(list)\n",
    "for exp in experiment_dict:\n",
    "    experiments_by_file[exp['path']].append(exp)\n",
    "\n",
    "# Process\n",
    "agg_stat_df = []\n",
    "for path, experiments in experiments_by_file.items():\n",
    "    df_full = cf.load_parquet_cached(path)\n",
    "    df_full = df_full[df_full['PG.Q.Value'] < 0.01]\n",
    "    \n",
    "    for experiment in experiments:\n",
    "        _df_agg = cf.process_experiment(\n",
    "            df_full, \n",
    "            experiment, \n",
    "            protease='trypsin',\n",
    "            max_missed_cleavages=2\n",
    "        )\n",
    "        agg_stat_df.append(_df_agg)\n",
    "\n",
    "df = pd.concat(agg_stat_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8998f39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Run",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "peptide",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "precursor",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "protein",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "MC0",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MC1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MC2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_MC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PG20",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Pr20",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_peptides",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_protein_groups",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_precursors",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "instrument",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "method",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "af65364f-9d79-400f-8667-63a163e1bf6a",
       "rows": [
        [
         "0",
         "20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_A1",
         "17546",
         "20670",
         "2056",
         "0.6193946188340808",
         "0.2897359242650722",
         "0.09086945690084704",
         "0.47147483806676627",
         "1170",
         "5833",
         "17800",
         "2295",
         "22880",
         "0.25 ng",
         "In sol"
        ],
        [
         "1",
         "20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_A2",
         "17650",
         "20839",
         "1971",
         "0.616822429906542",
         "0.2910812650863403",
         "0.09209630500711766",
         "0.4752738751005756",
         "1170",
         "5833",
         "17800",
         "2295",
         "22880",
         "0.25 ng",
         "In sol"
        ],
        [
         "2",
         "20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_A3",
         "17999",
         "21116",
         "2022",
         "0.6249697116549552",
         "0.28586140053307485",
         "0.08916888781196995",
         "0.4641991761570148",
         "1170",
         "5833",
         "17800",
         "2295",
         "22880",
         "0.25 ng",
         "In sol"
        ],
        [
         "3",
         "20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_A4",
         "16049",
         "18787",
         "1879",
         "0.631564607315578",
         "0.2800217968803215",
         "0.08841359580410053",
         "0.4568489884885226",
         "1170",
         "5833",
         "17800",
         "2295",
         "22880",
         "0.25 ng",
         "In sol"
        ],
        [
         "4",
         "20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_B1",
         "16825",
         "18834",
         "2690",
         "0.7403682986613035",
         "0.2145056878888819",
         "0.045126013449814595",
         "0.3047577147885111",
         "2175",
         "8874",
         "18953",
         "3136",
         "22463",
         "1 ng",
         "In sol"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>peptide</th>\n",
       "      <th>precursor</th>\n",
       "      <th>protein</th>\n",
       "      <th>MC0</th>\n",
       "      <th>MC1</th>\n",
       "      <th>MC2</th>\n",
       "      <th>avg_MC</th>\n",
       "      <th>PG20</th>\n",
       "      <th>Pr20</th>\n",
       "      <th>total_peptides</th>\n",
       "      <th>total_protein_groups</th>\n",
       "      <th>total_precursors</th>\n",
       "      <th>instrument</th>\n",
       "      <th>method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_A1</td>\n",
       "      <td>17546</td>\n",
       "      <td>20670</td>\n",
       "      <td>2056</td>\n",
       "      <td>0.619395</td>\n",
       "      <td>0.289736</td>\n",
       "      <td>0.090869</td>\n",
       "      <td>0.471475</td>\n",
       "      <td>1170</td>\n",
       "      <td>5833</td>\n",
       "      <td>17800</td>\n",
       "      <td>2295</td>\n",
       "      <td>22880</td>\n",
       "      <td>0.25 ng</td>\n",
       "      <td>In sol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_A2</td>\n",
       "      <td>17650</td>\n",
       "      <td>20839</td>\n",
       "      <td>1971</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.291081</td>\n",
       "      <td>0.092096</td>\n",
       "      <td>0.475274</td>\n",
       "      <td>1170</td>\n",
       "      <td>5833</td>\n",
       "      <td>17800</td>\n",
       "      <td>2295</td>\n",
       "      <td>22880</td>\n",
       "      <td>0.25 ng</td>\n",
       "      <td>In sol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_A3</td>\n",
       "      <td>17999</td>\n",
       "      <td>21116</td>\n",
       "      <td>2022</td>\n",
       "      <td>0.624970</td>\n",
       "      <td>0.285861</td>\n",
       "      <td>0.089169</td>\n",
       "      <td>0.464199</td>\n",
       "      <td>1170</td>\n",
       "      <td>5833</td>\n",
       "      <td>17800</td>\n",
       "      <td>2295</td>\n",
       "      <td>22880</td>\n",
       "      <td>0.25 ng</td>\n",
       "      <td>In sol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_A4</td>\n",
       "      <td>16049</td>\n",
       "      <td>18787</td>\n",
       "      <td>1879</td>\n",
       "      <td>0.631565</td>\n",
       "      <td>0.280022</td>\n",
       "      <td>0.088414</td>\n",
       "      <td>0.456849</td>\n",
       "      <td>1170</td>\n",
       "      <td>5833</td>\n",
       "      <td>17800</td>\n",
       "      <td>2295</td>\n",
       "      <td>22880</td>\n",
       "      <td>0.25 ng</td>\n",
       "      <td>In sol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_B1</td>\n",
       "      <td>16825</td>\n",
       "      <td>18834</td>\n",
       "      <td>2690</td>\n",
       "      <td>0.740368</td>\n",
       "      <td>0.214506</td>\n",
       "      <td>0.045126</td>\n",
       "      <td>0.304758</td>\n",
       "      <td>2175</td>\n",
       "      <td>8874</td>\n",
       "      <td>18953</td>\n",
       "      <td>3136</td>\n",
       "      <td>22463</td>\n",
       "      <td>1 ng</td>\n",
       "      <td>In sol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Run  peptide  precursor  \\\n",
       "0  20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_A1    17546      20670   \n",
       "1  20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_A2    17650      20839   \n",
       "2  20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_A3    17999      21116   \n",
       "3  20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_A4    16049      18787   \n",
       "4  20250524_OA4_Evo11_16p3_min_TiHe_SA_H032_E180_B1    16825      18834   \n",
       "\n",
       "   protein       MC0       MC1       MC2    avg_MC  PG20  Pr20  \\\n",
       "0     2056  0.619395  0.289736  0.090869  0.471475  1170  5833   \n",
       "1     1971  0.616822  0.291081  0.092096  0.475274  1170  5833   \n",
       "2     2022  0.624970  0.285861  0.089169  0.464199  1170  5833   \n",
       "3     1879  0.631565  0.280022  0.088414  0.456849  1170  5833   \n",
       "4     2690  0.740368  0.214506  0.045126  0.304758  2175  8874   \n",
       "\n",
       "   total_peptides  total_protein_groups  total_precursors instrument  method  \n",
       "0           17800                  2295             22880    0.25 ng  In sol  \n",
       "1           17800                  2295             22880    0.25 ng  In sol  \n",
       "2           17800                  2295             22880    0.25 ng  In sol  \n",
       "3           17800                  2295             22880    0.25 ng  In sol  \n",
       "4           18953                  3136             22463       1 ng  In sol  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e69af7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'Z:\\Tim_SPEC\\K562_dilution_series_SPEC_vs_InSol\\stats_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff8a8c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = df['instrument'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3063f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insol = df[df['method'] == 'In sol']\n",
    "df_SPEC = df[df['method'] == 'SPEC']\n",
    "df_PAC = df[df['method'] == 'PAC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd612116",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_insol = []\n",
    "for el in input_list:\n",
    "    tmp_df = df_insol[df_insol['instrument'] == el]\n",
    "    fin_insol.append(tmp_df['protein'].tolist()) \n",
    "\n",
    "\n",
    "flattened_insol = [item for sublist in fin_insol for item in sublist]\n",
    "input_list1 = np.repeat(input_list, 4).tolist()\n",
    "df_insol = pd.DataFrame({'Selectivity':flattened_insol, 'ID':input_list1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "708ab111",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_SPEC = []\n",
    "for el in input_list:\n",
    "    tmp_df = df_SPEC[df_SPEC['instrument'] == el]\n",
    "    fin_SPEC.append(tmp_df['protein'].tolist()) \n",
    "flattened_SPEC = [item for sublist in fin_SPEC for item in sublist]\n",
    "input_list1 = np.repeat(input_list, 4).tolist()\n",
    "df_SPEC = pd.DataFrame({'Selectivity':flattened_SPEC, 'ID':input_list1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb435282",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_PAC = []\n",
    "for el in input_list:\n",
    "    tmp_df = df_PAC[df_PAC['instrument'] == el]\n",
    "    fin_PAC.append(tmp_df['protein'].tolist()) \n",
    "flattened_PAC = [item for sublist in fin_PAC for item in sublist]\n",
    "input_list1 = np.repeat(input_list, 4).tolist()\n",
    "df_PAC = pd.DataFrame({'Selectivity':flattened_PAC, 'ID':input_list1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd65550",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(df_SPEC, y='Selectivity', x='ID', orientation='h')\n",
    "\n",
    "fig.add_scatter(\n",
    "    y=df_insol['Selectivity'], \n",
    "    x=df_insol['ID'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=14, color=' #faa307', line=dict(width=0.5, color='black')),\n",
    "    name='df_insol'\n",
    ")\n",
    "\n",
    "fig.add_scatter(\n",
    "    y=df_PAC['Selectivity'], \n",
    "    x=df_PAC['ID'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=14, color='#6D25AD', line=dict(width=0.5, color='black')),\n",
    "    name='df_PAC' \n",
    ")\n",
    "\n",
    "fig.update_layout(width=600, height=600, template='plotly_white', showlegend=False)\n",
    "\n",
    "\n",
    "fig.update_traces(\n",
    "    marker=dict(size=14, color='#db4c2e', line=dict(width=0.5, color='black')),\n",
    "    selector=dict(type='box')  \n",
    ")\n",
    "\n",
    "\n",
    "fig.update_yaxes(\n",
    "    range=[0, 10100],\n",
    "    showgrid=True,\n",
    "    gridwidth=0.1,           \n",
    "    gridcolor='#F3F2F2',  \n",
    "    griddash='solid',\n",
    "    title='Selectivity (%)'  \n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "    title='Sample ID' \n",
    ")\n",
    "\n",
    "fig.show()\n",
    "#fig.write_image(r'D:\\Projects\\SPEC\\figs_raw\\Figure2a.pdf', width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02259f50",
   "metadata": {},
   "source": [
    "# Figure 2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e8b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mTRAQ = pd.read_parquet(r'Z:\\Tim_SPEC\\mTRAQ\\mTRAQ.parquet')\n",
    "df_LFQ = pd.read_parquet(r'Z:\\Tim_SPEC\\mTRAQ\\LFQ.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a83849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LFQ['reduced_id'] = df_LFQ['Run'].apply(lambda x: x.split('_')[-1])\n",
    "df_mTRAQ['reduced_id'] = df_mTRAQ['Run'].apply(lambda x: x.split('_')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab5c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = ['B7', 'B8', 'B9', 'C7', 'C8', 'C9', 'D7', 'D8', 'D9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0c45db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LFQ = df_LFQ[df_LFQ['reduced_id'].isin(to_keep)]\n",
    "df_mTRAQ = df_mTRAQ[df_mTRAQ['reduced_id'].isin(to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9b3799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep1 = [['B7', 'B8', 'B9'], ['C7', 'C8', 'C9'], ['D7', 'D8', 'D9']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44451abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = []\n",
    "for el in to_keep1:\n",
    "    tmp = []\n",
    "    for el1 in el:\n",
    "        tmp_df1 = df_LFQ[df_LFQ['reduced_id'] == el1].drop_duplicates('Precursor.Id')\n",
    "        tmp_df2 = df_mTRAQ[df_mTRAQ['reduced_id'] == el1].drop_duplicates('Precursor.Id')\n",
    "        tmp.append(np.sum(tmp_df2['Precursor.Quantity'].tolist()) / (np.sum(tmp_df2['Precursor.Quantity'].tolist()) + np.sum(tmp_df1['Precursor.Quantity'].tolist())))\n",
    "    fin.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ceb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['d0','d4', 'd8']\n",
    "colors = ['#db93c0', '#b67ec3', '#9269c6']  \n",
    "\n",
    "medians = [np.median(reps) for reps in fin]\n",
    "\n",
    "strip_data = []\n",
    "for i, (label, reps) in enumerate(zip(labels, fin)):\n",
    "    for rep_val in reps:\n",
    "        strip_data.append({'Category': label, 'Value': rep_val})\n",
    "\n",
    "strip_df = pd.DataFrame(strip_data)\n",
    "\n",
    "fig = px.strip(strip_df, x='Category', y='Value', \n",
    "               category_orders={'Category': labels})\n",
    "\n",
    "fig.update_traces(\n",
    "    jitter=0.6,\n",
    "    marker=dict(size=18, color='black', line=dict(width=0.5, color='white'))\n",
    ")\n",
    "\n",
    "for i, (label, median, color) in enumerate(zip(labels, medians, colors)):\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=[label],\n",
    "        y=[median],\n",
    "        width=0.3,\n",
    "        marker=dict(color=color, line=dict(width=1, color='black')),\n",
    "        name=label,\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    width=600,\n",
    "    height=350,\n",
    "    template='plotly_white',\n",
    "    xaxis_title='',\n",
    "    yaxis_title='Phosphosites Identified',\n",
    "    yaxis=dict(range=[0, 1.1]),\n",
    "    showlegend=False\n",
    ")\n",
    "#fig.write_image(r'D:\\Projects\\SPEC\\figs_raw\\Figure_2l.pdf', width=600, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f15d87",
   "metadata": {},
   "source": [
    "# Figure 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a85bca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = set(df_mTRAQ[df_mTRAQ['reduced_id'].str.contains('B')].drop_duplicates('Protein.Group')['Protein.Group'])\n",
    "set2 = set(df_mTRAQ[df_mTRAQ['reduced_id'].str.contains('C')].drop_duplicates('Protein.Group')['Protein.Group'])\n",
    "set3 = set(df_mTRAQ[df_mTRAQ['reduced_id'].str.contains('D')].drop_duplicates('Protein.Group')['Protein.Group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "v = venn3([set1, set2, set3], \n",
    "          set_labels=('d0', 'd4', 'd8'),\n",
    "          set_colors=('#db93c0', '#b67ec3', '#9269c6'),\n",
    "          alpha=0.6)\n",
    "\n",
    "\n",
    "for text in v.set_labels:\n",
    "    text.set_fontsize(14)\n",
    "    text.set_fontweight('bold')\n",
    "\n",
    "for text in v.subset_labels:\n",
    "    if text:\n",
    "        text.set_fontsize(12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(r'D:\\Projects\\SPEC\\figs_raw\\Figure_2m.pdf', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568d72e",
   "metadata": {},
   "source": [
    "# Figure 2F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eaed39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r'Z:/Tim_SPEC/fractionation/LTH43_report.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d308bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['PG.Q.Value']<0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d02213e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['organism'] = df['Protein.Names'].apply(lambda x: x.split('_')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e689dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "organism_list = df['organism'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96b0bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [['A01', 'A02', 'A03'], ['A04', 'A05', 'A06'],  ['B04', 'B05', 'B06'],  ['C04', 'C05', 'C06'],\n",
    "           ['D04', 'D05', 'D06'], ['E04', 'E05', 'E06']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "554267b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reduced_id'] = df['Run'].apply(lambda x: x.split('_')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5abe7122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frac = df[df['reduced_id'].isin(groups[0]) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "165b00d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ss = df[df['reduced_id'].isin(groups[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "530036a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_ss = []\n",
    "for org in organism_list:\n",
    "    tmp = []\n",
    "    for el in df_ss['reduced_id'].unique().tolist():\n",
    "        tmp_df = df_ss[(df_ss['organism'] == org) & (df_ss['reduced_id'] == el)]\n",
    "        tmp.append(len(tmp_df.drop_duplicates('Protein.Group')))\n",
    "    nums_ss.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba45534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_frac = []\n",
    "reps = ['04', '05', '06']\n",
    "\n",
    "for org in organism_list:\n",
    "    tmp = []\n",
    "    for el in reps:\n",
    "        tmp_df = df_frac[(df_frac['organism'] == org) & (df_frac['reduced_id'].str.contains(el))]\n",
    "        tmp.append(len(tmp_df.drop_duplicates('Protein.Group')))\n",
    "    nums_frac.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "328b4b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_ss_prec = []\n",
    "for org in organism_list:\n",
    "    tmp = []\n",
    "    for el in df_ss['reduced_id'].unique().tolist():\n",
    "        tmp_df = df_ss[(df_ss['organism'] == org) & (df_ss['reduced_id'] == el)]\n",
    "        tmp.append(len(tmp_df.drop_duplicates('Precursor.Id')))\n",
    "    nums_ss_prec.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2cc48c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_frac_prec = []\n",
    "reps = ['04', '05', '06']\n",
    "\n",
    "for org in organism_list:\n",
    "    tmp = []\n",
    "    for el in reps:\n",
    "        tmp_df = df_frac[(df_frac['organism'] == org) & (df_frac['reduced_id'].str.contains(el))]\n",
    "        tmp.append(len(tmp_df.drop_duplicates('Precursor.Id')))\n",
    "    nums_frac_prec.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7857b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_prec_ss = 0\n",
    "for el in nums_ss_prec:\n",
    "    sum_prec_ss += np.median(el)\n",
    "####\n",
    "sum_pg_ss = 0\n",
    "for el in nums_ss:\n",
    "    sum_pg_ss += np.median(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fbde376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_prec_frac = 0\n",
    "for el in nums_frac_prec:\n",
    "    sum_prec_frac += np.median(el)\n",
    "####\n",
    "sum_pg_frac = 0\n",
    "for el in nums_frac:\n",
    "    sum_pg_frac += np.median(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da124b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "single_shot_reps = [[7314, 7263, 7123], [4355, 4399, 4217], [4200, 4185, 4137]]\n",
    "fractionation_reps = [[8812, 8921, 8871], [6953, 7182, 7162], [4707, 4727, 4724]]\n",
    "\n",
    "species = ['Species 1', 'Species 2', 'Species 3']\n",
    "colors = ['#00115e', '#004440', '#007621']\n",
    "\n",
    "\n",
    "single_medians = [np.median(reps) for reps in single_shot_reps]\n",
    "fract_medians = [np.median(reps) for reps in fractionation_reps]\n",
    "\n",
    "strip_data = []\n",
    "cum_single = np.cumsum([0] + single_medians[:-1])\n",
    "cum_fract = np.cumsum([0] + fract_medians[:-1])\n",
    "\n",
    "for i in range(len(species)):\n",
    "    for rep_s, rep_f in zip(single_shot_reps[i], fractionation_reps[i]):\n",
    "        strip_data.append({'Method': 'Single-shot', 'Value': cum_single[i] + rep_s})\n",
    "        strip_data.append({'Method': 'Fractionation', 'Value': cum_fract[i] + rep_f})\n",
    "\n",
    "strip_df = pd.DataFrame(strip_data)\n",
    "\n",
    "fig = px.strip(strip_df, y='Value', x='Method')\n",
    "fig.update_traces(jitter=0.5, marker=dict(size=14, color='black', \n",
    "                  line=dict(width=0.5, color='white')))\n",
    "\n",
    "# Add stacked bars\n",
    "for i, (sp, color) in enumerate(zip(species, colors)):\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=[single_medians[i]], \n",
    "        x=['Single-shot'], \n",
    "        width=0.4, \n",
    "        marker_line_color='black', \n",
    "        marker_color=color,\n",
    "        name=sp\n",
    "    ))\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=[fract_medians[i]], \n",
    "        x=['Fractionation'], \n",
    "        width=0.4, \n",
    "        marker_line_color='black', \n",
    "        marker_color=color,\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    width=500, \n",
    "    height=700, \n",
    "    template='plotly_white', \n",
    "    barmode='stack',\n",
    "    xaxis_title='Method',\n",
    "    yaxis_title='Phosphosites Identified',\n",
    "    legend_title='Species'\n",
    ")\n",
    "#fig.write_image(r'D:\\Projects\\SPEC\\figs_raw\\Figure_2c.pdf', width = 500, height = 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a66dbb",
   "metadata": {},
   "source": [
    "# Figure 2G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7260d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "single_shot_reps = nums_ss_prec\n",
    "fractionation_reps = nums_frac_prec\n",
    "\n",
    "species = ['Species 1', 'Species 2', 'Species 3']\n",
    "colors = ['#5a6776', '#916e75', '#e47874']\n",
    "\n",
    "\n",
    "single_medians = [np.median(reps) for reps in single_shot_reps]\n",
    "fract_medians = [np.median(reps) for reps in fractionation_reps]\n",
    "\n",
    "strip_data = []\n",
    "cum_single = np.cumsum([0] + single_medians[:-1])\n",
    "cum_fract = np.cumsum([0] + fract_medians[:-1])\n",
    "\n",
    "for i in range(len(species)):\n",
    "    for rep_s, rep_f in zip(single_shot_reps[i], fractionation_reps[i]):\n",
    "        strip_data.append({'Method': 'Single-shot', 'Value': cum_single[i] + rep_s})\n",
    "        strip_data.append({'Method': 'Fractionation', 'Value': cum_fract[i] + rep_f})\n",
    "\n",
    "strip_df = pd.DataFrame(strip_data)\n",
    "\n",
    "fig = px.strip(strip_df, y='Value', x='Method')\n",
    "fig.update_traces(jitter=0.5, marker=dict(size=14, color='black', \n",
    "                  line=dict(width=0.5, color='white')))\n",
    "\n",
    "# Add stacked bars\n",
    "for i, (sp, color) in enumerate(zip(species, colors)):\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=[single_medians[i]], \n",
    "        x=['Single-shot'], \n",
    "        width=0.4, \n",
    "        marker_line_color='black', \n",
    "        marker_color=color,\n",
    "        name=sp\n",
    "    ))\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=[fract_medians[i]], \n",
    "        x=['Fractionation'], \n",
    "        width=0.4, \n",
    "        marker_line_color='black', \n",
    "        marker_color=color,\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    width=500, \n",
    "    height=700, \n",
    "    template='plotly_white', \n",
    "    barmode='stack',\n",
    "    xaxis_title='Method',\n",
    "    yaxis_title='Phosphosites Identified',\n",
    "    legend_title='Species'\n",
    ")\n",
    "#fig.write_image(r'D:\\Projects\\SPEC\\figs_raw\\Figure_2d.pdf', width = 500, height = 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772d972",
   "metadata": {},
   "source": [
    "# Figure 2H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_unique_identifiers_across_methods(experiment_dict: List[Dict],\n",
    "                                             q_value_threshold: float = 0.01,\n",
    "                                             output_dir: str = None,\n",
    "                                             figure_size: tuple = (12, 8),\n",
    "                                             vector_format: str = 'svg',\n",
    "                                             save_raster: bool = True,\n",
    "                                             export_identifier_lists: bool = False) -> Tuple[pd.DataFrame, plt.Figure]:\n",
    "    \"\"\"\n",
    "    Compare unique precursors and proteins across different method groups with vector output.\n",
    "    \n",
    "    This function aggregates unique identifiers from conditions belonging to\n",
    "    each method (e.g., all fractionation conditions vs non-fractionation). For each\n",
    "    condition, only identifiers found in ALL replicates within that condition are kept,\n",
    "    then these filtered sets are merged across conditions within each method group.\n",
    "    \n",
    "    For example, with fractionation: an identifier must be found in all 3 replicates\n",
    "    within at least one fraction to be included in the fractionation method group.\n",
    "    \n",
    "    The function exports:\n",
    "    - Summary tables (always exported when output_dir is set)\n",
    "    - Filtered data files (.parquet and .csv) containing all rows that passed filtering (always)\n",
    "    - Identifier lists (.txt) for shared/unique identifiers (when export_identifier_lists=True)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    experiment_dict : List[Dict]\n",
    "        List of experiment dictionaries with 'method' field\n",
    "    q_value_threshold : float, default=0.01\n",
    "        Q-value threshold for filtering\n",
    "    output_dir : str, optional\n",
    "        Directory to save the output figure\n",
    "    figure_size : tuple, optional\n",
    "        Size of the figure (width, height) in inches\n",
    "    vector_format : str, default='svg'\n",
    "        Vector format for saving ('svg', 'pdf', 'eps')\n",
    "    save_raster : bool, default=True\n",
    "        Whether to also save high-resolution raster version\n",
    "    export_identifier_lists : bool, default=False\n",
    "        Whether to export detailed lists of identifiers for each method group\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (summary_df, fig[, fig_venn]) - DataFrame with unique counts and the comparison figure(s)\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import defaultdict\n",
    "    import os\n",
    "    \n",
    "    # Set matplotlib to use vector-friendly settings\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'DejaVu Sans',\n",
    "        'font.size': 12,\n",
    "        'axes.titlesize': 18,\n",
    "        'axes.labelsize': 16,\n",
    "        'xtick.labelsize': 14,\n",
    "        'ytick.labelsize': 14,\n",
    "        'legend.fontsize': 14,\n",
    "        'figure.titlesize': 20,\n",
    "        'axes.linewidth': 1.5,\n",
    "        'grid.linewidth': 1.0,\n",
    "        'lines.linewidth': 2.0,\n",
    "        'patch.linewidth': 1.5,\n",
    "        'savefig.facecolor': 'white',\n",
    "        'savefig.edgecolor': 'none',\n",
    "        'savefig.dpi': 300 if save_raster else 100\n",
    "    })\n",
    "    \n",
    "    # Group experiments by method\n",
    "    method_groups = defaultdict(list)\n",
    "    for exp in experiment_dict:\n",
    "        method = exp['method'] if exp['method'] else 'no_fractionation'\n",
    "        method_groups[method].append(exp)\n",
    "    \n",
    "    print(f\"Found {len(method_groups)} method groups: {list(method_groups.keys())}\")\n",
    "    print(f\"Filtering strategy: identifiers must be found in ALL replicates within at least one condition\")\n",
    "    print(f\"Vector format: {vector_format.upper()}\")\n",
    "    \n",
    "    # Collect unique identifiers for each method group\n",
    "    method_identifiers = {}\n",
    "    \n",
    "    for method, experiments in method_groups.items():\n",
    "        print(f\"\\nProcessing method group: '{method}' with {len(experiments)} experiments\")\n",
    "        \n",
    "        # Collect filtered identifiers from each condition separately\n",
    "        method_precursors = set()\n",
    "        method_proteins = set()\n",
    "        method_peptides = set()\n",
    "        \n",
    "        # Store filtered dataframes for export\n",
    "        method_filtered_dfs = []\n",
    "        \n",
    "        total_conditions = 0\n",
    "        \n",
    "        # Group by file to read each file only once\n",
    "        file_experiments = defaultdict(list)\n",
    "        for exp in experiments:\n",
    "            file_experiments[exp['path']].append(exp)\n",
    "        \n",
    "        for file_path, file_exps in file_experiments.items():\n",
    "            try:\n",
    "                print(f\"  Reading {file_path}...\")\n",
    "                df = pd.read_parquet(file_path)\n",
    "                \n",
    "                # Apply Q-value filter\n",
    "                df = df[df['PG.Q.Value'] < q_value_threshold]\n",
    "                \n",
    "                if df.empty:\n",
    "                    print(f\"  Warning: No data after Q-value filtering\")\n",
    "                    continue\n",
    "                \n",
    "                # Process each condition separately\n",
    "                for exp in file_exps:\n",
    "                    total_conditions += 1\n",
    "                    \n",
    "                    # Create tag pattern for filtering\n",
    "                    tag_pattern = '|'.join(exp['file_tags'])\n",
    "                    mask = df['Run'].str.contains(tag_pattern, na=False, regex=True)\n",
    "                    exp_df = df[mask]\n",
    "                    \n",
    "                    if exp_df.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get unique runs for this condition\n",
    "                    runs_in_condition = exp_df['Run'].unique()\n",
    "                    n_replicates = len(runs_in_condition)\n",
    "                    \n",
    "                    # Track which runs each identifier appears in within this condition\n",
    "                    condition_precursor_runs = defaultdict(set)\n",
    "                    condition_protein_runs = defaultdict(set)\n",
    "                    condition_peptide_runs = defaultdict(set)\n",
    "                    \n",
    "                    for run in runs_in_condition:\n",
    "                        run_df = exp_df[exp_df['Run'] == run]\n",
    "                        \n",
    "                        for precursor in run_df['Precursor.Id'].unique():\n",
    "                            condition_precursor_runs[precursor].add(run)\n",
    "                        \n",
    "                        for protein in run_df['Protein.Group'].unique():\n",
    "                            condition_protein_runs[protein].add(run)\n",
    "                        \n",
    "                        for peptide in run_df['Stripped.Sequence'].unique():\n",
    "                            condition_peptide_runs[peptide].add(run)\n",
    "                    \n",
    "                    # Filter for identifiers found in all replicates in this condition\n",
    "                    condition_filtered_precursors = {p for p, runs in condition_precursor_runs.items() \n",
    "                                                    if len(runs) == n_replicates}\n",
    "                    condition_filtered_proteins = {p for p, runs in condition_protein_runs.items() \n",
    "                                                  if len(runs) == n_replicates}\n",
    "                    condition_filtered_peptides = {p for p, runs in condition_peptide_runs.items() \n",
    "                                                  if len(runs) == n_replicates}\n",
    "                    \n",
    "                    # Filter the dataframe to only include identifiers found in all replicates\n",
    "                    condition_filtered_df = exp_df[exp_df['Precursor.Id'].isin(condition_filtered_precursors)]\n",
    "                    method_filtered_dfs.append(condition_filtered_df)\n",
    "                    \n",
    "                    condition_name = f\"{exp['instrument']}_{exp.get('condition', 'condition')}\"\n",
    "                    print(f\"    {condition_name}: {n_replicates} replicates\")\n",
    "                    print(f\"      Before filtering: {len(condition_precursor_runs)} precursors, \"\n",
    "                          f\"{len(condition_protein_runs)} proteins, \"\n",
    "                          f\"{len(condition_peptide_runs)} peptides\")\n",
    "                    print(f\"      After filtering (found in all {n_replicates} replicates): \"\n",
    "                          f\"{len(condition_filtered_precursors)} precursors, \"\n",
    "                          f\"{len(condition_filtered_proteins)} proteins, \"\n",
    "                          f\"{len(condition_filtered_peptides)} peptides\")\n",
    "                    print(f\"      Filtered dataframe rows: {len(condition_filtered_df)}\")\n",
    "                    \n",
    "                    # Add to method group's aggregated identifiers\n",
    "                    method_precursors.update(condition_filtered_precursors)\n",
    "                    method_proteins.update(condition_filtered_proteins)\n",
    "                    method_peptides.update(condition_filtered_peptides)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error reading {file_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n  Total conditions in method group '{method}': {total_conditions}\")\n",
    "        print(f\"  Final aggregated counts for '{method}': \"\n",
    "              f\"{len(method_precursors)} precursors, \"\n",
    "              f\"{len(method_proteins)} proteins, \"\n",
    "              f\"{len(method_peptides)} peptides\")\n",
    "        \n",
    "        # Concatenate all filtered dataframes for this method\n",
    "        if method_filtered_dfs:\n",
    "            combined_df = pd.concat(method_filtered_dfs, ignore_index=True)\n",
    "            print(f\"  Combined filtered dataframe: {len(combined_df)} total rows\")\n",
    "        else:\n",
    "            combined_df = pd.DataFrame()\n",
    "        \n",
    "        method_identifiers[method] = {\n",
    "            'precursors': method_precursors,\n",
    "            'proteins': method_proteins,\n",
    "            'peptides': method_peptides,\n",
    "            'filtered_df': combined_df\n",
    "        }\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    summary_data = []\n",
    "    for method, identifiers in method_identifiers.items():\n",
    "        summary_data.append({\n",
    "            'Method': method,\n",
    "            'Unique_Precursors': len(identifiers['precursors']),\n",
    "            'Unique_Proteins': len(identifiers['proteins']),\n",
    "            'Unique_Peptides': len(identifiers['peptides'])\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Calculate overlaps if we have exactly 2 method groups\n",
    "    if len(method_groups) == 2:\n",
    "        methods = list(method_identifiers.keys())\n",
    "        m1, m2 = methods[0], methods[1]\n",
    "        \n",
    "        # Calculate overlaps\n",
    "        precursor_overlap = len(method_identifiers[m1]['precursors'] & \n",
    "                               method_identifiers[m2]['precursors'])\n",
    "        protein_overlap = len(method_identifiers[m1]['proteins'] & \n",
    "                             method_identifiers[m2]['proteins'])\n",
    "        peptide_overlap = len(method_identifiers[m1]['peptides'] & \n",
    "                             method_identifiers[m2]['peptides'])\n",
    "        \n",
    "        # Calculate unique to each method\n",
    "        precursor_only_m1 = len(method_identifiers[m1]['precursors'] - \n",
    "                                method_identifiers[m2]['precursors'])\n",
    "        precursor_only_m2 = len(method_identifiers[m2]['precursors'] - \n",
    "                                method_identifiers[m1]['precursors'])\n",
    "        \n",
    "        protein_only_m1 = len(method_identifiers[m1]['proteins'] - \n",
    "                              method_identifiers[m2]['proteins'])\n",
    "        protein_only_m2 = len(method_identifiers[m2]['proteins'] - \n",
    "                              method_identifiers[m1]['proteins'])\n",
    "        \n",
    "        print(f\"\\n=== Overlap Analysis ===\")\n",
    "        print(f\"Precursors: {precursor_overlap} shared, \"\n",
    "              f\"{precursor_only_m1} unique to {m1}, \"\n",
    "              f\"{precursor_only_m2} unique to {m2}\")\n",
    "        print(f\"Proteins: {protein_overlap} shared, \"\n",
    "              f\"{protein_only_m1} unique to {m1}, \"\n",
    "              f\"{protein_only_m2} unique to {m2}\")\n",
    "        \n",
    "        # Create overlap summary DataFrame\n",
    "        overlap_df = pd.DataFrame([\n",
    "            {\n",
    "                'Identifier_Type': 'Precursors',\n",
    "                'Shared': precursor_overlap,\n",
    "                f'Unique_to_{m1}': precursor_only_m1,\n",
    "                f'Unique_to_{m2}': precursor_only_m2,\n",
    "                f'Total_{m1}': len(method_identifiers[m1]['precursors']),\n",
    "                f'Total_{m2}': len(method_identifiers[m2]['precursors'])\n",
    "            },\n",
    "            {\n",
    "                'Identifier_Type': 'Proteins',\n",
    "                'Shared': protein_overlap,\n",
    "                f'Unique_to_{m1}': protein_only_m1,\n",
    "                f'Unique_to_{m2}': protein_only_m2,\n",
    "                f'Total_{m1}': len(method_identifiers[m1]['proteins']),\n",
    "                f'Total_{m2}': len(method_identifiers[m2]['proteins'])\n",
    "            },\n",
    "            {\n",
    "                'Identifier_Type': 'Peptides',\n",
    "                'Shared': len(method_identifiers[m1]['peptides'] & method_identifiers[m2]['peptides']),\n",
    "                f'Unique_to_{m1}': len(method_identifiers[m1]['peptides'] - method_identifiers[m2]['peptides']),\n",
    "                f'Unique_to_{m2}': len(method_identifiers[m2]['peptides'] - method_identifiers[m1]['peptides']),\n",
    "                f'Total_{m1}': len(method_identifiers[m1]['peptides']),\n",
    "                f'Total_{m2}': len(method_identifiers[m2]['peptides'])\n",
    "            }\n",
    "        ])\n",
    "    \n",
    "    # Create comparison plot with enhanced styling\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figure_size)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # Define professional color palette\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E', '#8E44AD', '#E67E22']\n",
    "    method_colors = {method: colors[i % len(colors)] \n",
    "                    for i, method in enumerate(summary_df['Method'])}\n",
    "    \n",
    "    # Plot 1: Precursors\n",
    "    ax1 = axes[0]\n",
    "    bars1 = ax1.bar(summary_df['Method'], summary_df['Unique_Precursors'],\n",
    "                    color=[method_colors[m] for m in summary_df['Method']],\n",
    "                    edgecolor='white', linewidth=2, alpha=0.9)\n",
    "    ax1.set_ylabel('Unique Count', fontsize=16, fontweight='bold')\n",
    "    ax1.set_title('Unique Precursors', fontsize=18, fontweight='bold', pad=20)\n",
    "    ax1.grid(axis='y', alpha=0.3, linestyle='--', linewidth=1)\n",
    "    ax1.set_axisbelow(True)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.tick_params(axis='both', labelsize=14, width=1.5)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars1, summary_df['Unique_Precursors']):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + max(summary_df['Unique_Precursors'])*0.02,\n",
    "                f'{value:,}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Proteins\n",
    "    ax2 = axes[1]\n",
    "    bars2 = ax2.bar(summary_df['Method'], summary_df['Unique_Proteins'],\n",
    "                    color=[method_colors[m] for m in summary_df['Method']],\n",
    "                    edgecolor='white', linewidth=2, alpha=0.9)\n",
    "    ax2.set_ylabel('Unique Count', fontsize=16, fontweight='bold')\n",
    "    ax2.set_title('Unique Proteins', fontsize=18, fontweight='bold', pad=20)\n",
    "    ax2.grid(axis='y', alpha=0.3, linestyle='--', linewidth=1)\n",
    "    ax2.set_axisbelow(True)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.tick_params(axis='both', labelsize=14, width=1.5)\n",
    "    \n",
    "    for bar, value in zip(bars2, summary_df['Unique_Proteins']):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + max(summary_df['Unique_Proteins'])*0.02,\n",
    "                f'{value:,}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Plot 3: Peptides\n",
    "    ax3 = axes[2]\n",
    "    bars3 = ax3.bar(summary_df['Method'], summary_df['Unique_Peptides'],\n",
    "                    color=[method_colors[m] for m in summary_df['Method']],\n",
    "                    edgecolor='white', linewidth=2, alpha=0.9)\n",
    "    ax3.set_ylabel('Unique Count', fontsize=16, fontweight='bold')\n",
    "    ax3.set_title('Unique Peptides', fontsize=18, fontweight='bold', pad=20)\n",
    "    ax3.grid(axis='y', alpha=0.3, linestyle='--', linewidth=1)\n",
    "    ax3.set_axisbelow(True)\n",
    "    ax3.spines['top'].set_visible(False)\n",
    "    ax3.spines['right'].set_visible(False)\n",
    "    ax3.tick_params(axis='both', labelsize=14, width=1.5)\n",
    "    \n",
    "    for bar, value in zip(bars3, summary_df['Unique_Peptides']):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + max(summary_df['Unique_Peptides'])*0.02,\n",
    "                f'{value:,}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Rotate x-axis labels if needed\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Method Group', fontsize=16, fontweight='bold')\n",
    "        if len(summary_df) > 2 or any(len(method) > 10 for method in summary_df['Method']):\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=14)\n",
    "    \n",
    "    plt.suptitle(f'Comparison of Unique Identifiers Across Method Groups\\n(Found in all replicates within at least one condition)', \n",
    "                 fontsize=20, fontweight='bold', y=1.08)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure(s) and summary table if output directory is provided\n",
    "    saved_files = []\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save summary table\n",
    "        table_path = os.path.join(output_dir, 'unique_identifiers_summary.csv')\n",
    "        summary_df.to_csv(table_path, index=False)\n",
    "        saved_files.append(table_path)\n",
    "        print(f\"\\nSaved summary table: {table_path}\")\n",
    "        \n",
    "        # Save overlap analysis if we have 2 method groups\n",
    "        if len(method_groups) == 2:\n",
    "            overlap_table_path = os.path.join(output_dir, 'overlap_analysis.csv')\n",
    "            overlap_df.to_csv(overlap_table_path, index=False)\n",
    "            saved_files.append(overlap_table_path)\n",
    "            print(f\"Saved overlap analysis: {overlap_table_path}\")\n",
    "            \n",
    "            # Export overlap/unique identifier lists if requested\n",
    "            if export_identifier_lists:\n",
    "                methods = list(method_identifiers.keys())\n",
    "                m1, m2 = methods[0], methods[1]\n",
    "                m1_safe = m1.replace('/', '_').replace(' ', '_')\n",
    "                m2_safe = m2.replace('/', '_').replace(' ', '_')\n",
    "                \n",
    "                print(\"\\nExporting overlap/unique identifier lists...\")\n",
    "                \n",
    "                # Shared identifiers\n",
    "                shared_precursors = method_identifiers[m1]['precursors'] & method_identifiers[m2]['precursors']\n",
    "                shared_path = os.path.join(output_dir, 'shared_precursors.txt')\n",
    "                with open(shared_path, 'w') as f:\n",
    "                    f.write('\\n'.join(sorted(shared_precursors)))\n",
    "                saved_files.append(shared_path)\n",
    "                print(f\"  Saved shared precursors: {shared_path}\")\n",
    "                \n",
    "                shared_proteins = method_identifiers[m1]['proteins'] & method_identifiers[m2]['proteins']\n",
    "                shared_path = os.path.join(output_dir, 'shared_proteins.txt')\n",
    "                with open(shared_path, 'w') as f:\n",
    "                    f.write('\\n'.join(sorted(shared_proteins)))\n",
    "                saved_files.append(shared_path)\n",
    "                print(f\"  Saved shared proteins: {shared_path}\")\n",
    "                \n",
    "                # Unique to m1\n",
    "                unique_m1_precursors = method_identifiers[m1]['precursors'] - method_identifiers[m2]['precursors']\n",
    "                unique_path = os.path.join(output_dir, f'unique_to_{m1_safe}_precursors.txt')\n",
    "                with open(unique_path, 'w') as f:\n",
    "                    f.write('\\n'.join(sorted(unique_m1_precursors)))\n",
    "                saved_files.append(unique_path)\n",
    "                print(f\"  Saved unique to {m1} precursors: {unique_path}\")\n",
    "                \n",
    "                unique_m1_proteins = method_identifiers[m1]['proteins'] - method_identifiers[m2]['proteins']\n",
    "                unique_path = os.path.join(output_dir, f'unique_to_{m1_safe}_proteins.txt')\n",
    "                with open(unique_path, 'w') as f:\n",
    "                    f.write('\\n'.join(sorted(unique_m1_proteins)))\n",
    "                saved_files.append(unique_path)\n",
    "                print(f\"  Saved unique to {m1} proteins: {unique_path}\")\n",
    "                \n",
    "                # Unique to m2\n",
    "                unique_m2_precursors = method_identifiers[m2]['precursors'] - method_identifiers[m1]['precursors']\n",
    "                unique_path = os.path.join(output_dir, f'unique_to_{m2_safe}_precursors.txt')\n",
    "                with open(unique_path, 'w') as f:\n",
    "                    f.write('\\n'.join(sorted(unique_m2_precursors)))\n",
    "                saved_files.append(unique_path)\n",
    "                print(f\"  Saved unique to {m2} precursors: {unique_path}\")\n",
    "                \n",
    "                unique_m2_proteins = method_identifiers[m2]['proteins'] - method_identifiers[m1]['proteins']\n",
    "                unique_path = os.path.join(output_dir, f'unique_to_{m2_safe}_proteins.txt')\n",
    "                with open(unique_path, 'w') as f:\n",
    "                    f.write('\\n'.join(sorted(unique_m2_proteins)))\n",
    "                saved_files.append(unique_path)\n",
    "                print(f\"  Saved unique to {m2} proteins: {unique_path}\")\n",
    "        \n",
    "        # Save detailed identifier lists for all methods if requested\n",
    "        if export_identifier_lists:\n",
    "            print(\"\\nExporting complete identifier lists per method...\")\n",
    "            for method, identifiers in method_identifiers.items():\n",
    "                method_safe = method.replace('/', '_').replace(' ', '_')\n",
    "                \n",
    "                # Export precursors\n",
    "                precursor_path = os.path.join(output_dir, f'{method_safe}_all_precursors.txt')\n",
    "                with open(precursor_path, 'w') as f:\n",
    "                    f.write('\\n'.join(sorted(identifiers['precursors'])))\n",
    "                saved_files.append(precursor_path)\n",
    "                print(f\"  Saved {method} all precursors: {precursor_path}\")\n",
    "                \n",
    "                # Export proteins\n",
    "                protein_path = os.path.join(output_dir, f'{method_safe}_all_proteins.txt')\n",
    "                with open(protein_path, 'w') as f:\n",
    "                    f.write('\\n'.join(sorted(identifiers['proteins'])))\n",
    "                saved_files.append(protein_path)\n",
    "                print(f\"  Saved {method} all proteins: {protein_path}\")\n",
    "                \n",
    "                # Export peptides\n",
    "                peptide_path = os.path.join(output_dir, f'{method_safe}_all_peptides.txt')\n",
    "                with open(peptide_path, 'w') as f:\n",
    "                    f.write('\\n'.join(sorted(identifiers['peptides'])))\n",
    "                saved_files.append(peptide_path)\n",
    "                print(f\"  Saved {method} all peptides: {peptide_path}\")\n",
    "        \n",
    "        # Export filtered dataframes (full data with all columns)\n",
    "        print(\"\\nExporting filtered dataframes...\")\n",
    "        for method, identifiers in method_identifiers.items():\n",
    "            if not identifiers['filtered_df'].empty:\n",
    "                method_safe = method.replace('/', '_').replace(' ', '_')\n",
    "                \n",
    "                # Export as parquet (preserves data types and is compressed)\n",
    "                parquet_path = os.path.join(output_dir, f'{method_safe}_filtered_data.parquet')\n",
    "                identifiers['filtered_df'].to_parquet(parquet_path, index=False)\n",
    "                saved_files.append(parquet_path)\n",
    "                print(f\"  Saved {method} filtered data: {parquet_path} ({len(identifiers['filtered_df'])} rows)\")\n",
    "                \n",
    "                # Also export as CSV for easier viewing\n",
    "                csv_path = os.path.join(output_dir, f'{method_safe}_filtered_data.csv')\n",
    "                identifiers['filtered_df'].to_csv(csv_path, index=False)\n",
    "                saved_files.append(csv_path)\n",
    "                print(f\"  Saved {method} filtered data (CSV): {csv_path}\")\n",
    "        \n",
    "        # Save vector format\n",
    "        vector_path = os.path.join(output_dir, f'unique_identifiers_comparison.{vector_format}')\n",
    "        fig.savefig(vector_path, format=vector_format, bbox_inches='tight', \n",
    "                   facecolor='white', edgecolor='none')\n",
    "        saved_files.append(vector_path)\n",
    "        print(f\"\\nSaved vector plot: {vector_path}\")\n",
    "        \n",
    "        # Save high-resolution raster if requested\n",
    "        if save_raster:\n",
    "            raster_path = os.path.join(output_dir, 'unique_identifiers_comparison.png')\n",
    "            fig.savefig(raster_path, dpi=300, bbox_inches='tight', \n",
    "                       facecolor='white', edgecolor='none')\n",
    "            saved_files.append(raster_path)\n",
    "            print(f\"Saved raster plot: {raster_path}\")\n",
    "    \n",
    "    # If we have exactly 2 groups, create a Venn diagram\n",
    "    if len(method_groups) == 2:\n",
    "        fig_venn, venn_files = create_venn_diagram(method_identifiers, output_dir, \n",
    "                                                   vector_format, save_raster)\n",
    "        saved_files.extend(venn_files)\n",
    "        return summary_df, fig, fig_venn, saved_files, method_identifiers\n",
    "    \n",
    "    return summary_df, fig, saved_files, method_identifiers\n",
    "\n",
    "\n",
    "def create_venn_diagram(method_identifiers: Dict, output_dir: str = None, \n",
    "                       vector_format: str = 'svg', save_raster: bool = True) -> Tuple[plt.Figure, List[str]]:\n",
    "    \"\"\"\n",
    "    Create Venn diagrams showing the overlap between two method groups with vector output.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    method_identifiers : Dict\n",
    "        Dictionary with method names as keys and identifier sets as values\n",
    "    output_dir : str, optional\n",
    "        Directory to save the output figure\n",
    "    vector_format : str, default='svg'\n",
    "        Vector format for saving ('svg', 'pdf', 'eps')\n",
    "    save_raster : bool, default=True\n",
    "        Whether to also save high-resolution raster version\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (fig, saved_files) - The created Venn diagram figure and list of saved file paths\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from matplotlib_venn import venn2\n",
    "    except ImportError:\n",
    "        print(\"Warning: matplotlib_venn not installed. Install with: pip install matplotlib-venn\")\n",
    "        return None, []\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    methods = list(method_identifiers.keys())\n",
    "    m1, m2 = methods[0], methods[1]\n",
    "    \n",
    "    # Professional colors for Venn diagrams\n",
    "    colors = ('#2E86AB', '#A23B72')\n",
    "    \n",
    "    # Precursors Venn\n",
    "    ax1 = axes[0]\n",
    "    try:\n",
    "        venn1 = venn2([method_identifiers[m1]['precursors'], \n",
    "                       method_identifiers[m2]['precursors']], \n",
    "                      set_labels=(m1, m2), ax=ax1,\n",
    "                      set_colors=colors, alpha=0.7)\n",
    "        ax1.set_title('Precursors Overlap', fontsize=20, fontweight='bold', pad=20)\n",
    "        \n",
    "        # Style Venn diagram text\n",
    "        if venn1.set_labels:\n",
    "            for text in venn1.set_labels:\n",
    "                if text:\n",
    "                    text.set_fontsize(16)\n",
    "                    text.set_fontweight('bold')\n",
    "        if venn1.subset_labels:\n",
    "            for text in venn1.subset_labels:\n",
    "                if text:\n",
    "                    text.set_fontsize(14)\n",
    "                    text.set_fontweight('bold')\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating precursors Venn diagram: {e}\")\n",
    "        ax1.text(0.5, 0.5, 'Error creating\\nVenn diagram', \n",
    "                ha='center', va='center', transform=ax1.transAxes, fontsize=14)\n",
    "        ax1.set_title('Precursors Overlap', fontsize=20, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Proteins Venn\n",
    "    ax2 = axes[1]\n",
    "    try:\n",
    "        venn2_plot = venn2([method_identifiers[m1]['proteins'], \n",
    "                            method_identifiers[m2]['proteins']], \n",
    "                           set_labels=(m1, m2), ax=ax2,\n",
    "                           set_colors=colors, alpha=0.7)\n",
    "        ax2.set_title('Proteins Overlap', fontsize=20, fontweight='bold', pad=20)\n",
    "        \n",
    "        if venn2_plot.set_labels:\n",
    "            for text in venn2_plot.set_labels:\n",
    "                if text:\n",
    "                    text.set_fontsize(16)\n",
    "                    text.set_fontweight('bold')\n",
    "        if venn2_plot.subset_labels:\n",
    "            for text in venn2_plot.subset_labels:\n",
    "                if text:\n",
    "                    text.set_fontsize(14)\n",
    "                    text.set_fontweight('bold')\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating proteins Venn diagram: {e}\")\n",
    "        ax2.text(0.5, 0.5, 'Error creating\\nVenn diagram', \n",
    "                ha='center', va='center', transform=ax2.transAxes, fontsize=14)\n",
    "        ax2.set_title('Proteins Overlap', fontsize=20, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Peptides Venn\n",
    "    ax3 = axes[2]\n",
    "    try:\n",
    "        venn3 = venn2([method_identifiers[m1]['peptides'], \n",
    "                       method_identifiers[m2]['peptides']], \n",
    "                      set_labels=(m1, m2), ax=ax3,\n",
    "                      set_colors=colors, alpha=0.7)\n",
    "        ax3.set_title('Peptides Overlap', fontsize=20, fontweight='bold', pad=20)\n",
    "        \n",
    "        if venn3.set_labels:\n",
    "            for text in venn3.set_labels:\n",
    "                if text:\n",
    "                    text.set_fontsize(16)\n",
    "                    text.set_fontweight('bold')\n",
    "        if venn3.subset_labels:\n",
    "            for text in venn3.subset_labels:\n",
    "                if text:\n",
    "                    text.set_fontsize(14)\n",
    "                    text.set_fontweight('bold')\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating peptides Venn diagram: {e}\")\n",
    "        ax3.text(0.5, 0.5, 'Error creating\\nVenn diagram', \n",
    "                ha='center', va='center', transform=ax3.transAxes, fontsize=14)\n",
    "        ax3.set_title('Peptides Overlap', fontsize=20, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.suptitle(f'Overlap Analysis Between Method Groups\\n(Identifiers found in all replicates within at least one condition)', \n",
    "                 fontsize=22, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure(s) if output directory is provided\n",
    "    saved_files = []\n",
    "    if output_dir:\n",
    "        import os\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save vector format\n",
    "        vector_path = os.path.join(output_dir, f'venn_diagram_comparison.{vector_format}')\n",
    "        fig.savefig(vector_path, format=vector_format, bbox_inches='tight',\n",
    "                   facecolor='white', edgecolor='none')\n",
    "        saved_files.append(vector_path)\n",
    "        print(f\"Saved Venn vector plot: {vector_path}\")\n",
    "        \n",
    "        # Save high-resolution raster if requested\n",
    "        if save_raster:\n",
    "            raster_path = os.path.join(output_dir, 'venn_diagram_comparison.png')\n",
    "            fig.savefig(raster_path, dpi=300, bbox_inches='tight',\n",
    "                       facecolor='white', edgecolor='none')\n",
    "            saved_files.append(raster_path)\n",
    "            print(f\"Saved Venn raster plot: {raster_path}\")\n",
    "    \n",
    "    return fig, saved_files\n",
    "def analyze_species_coverage(method_identifiers,\n",
    "                           fasta_human,\n",
    "                           fasta_yeast,\n",
    "                           fasta_arath,\n",
    "                           output_dir=None,\n",
    "                           figure_size=(16, 10),\n",
    "                           vector_format='svg',\n",
    "                           save_raster=True):\n",
    "    \"\"\"\n",
    "    Analyze peptide coverage per protein for each species in fractionated and non-fractionated methods.\n",
    "    \n",
    "    Creates histograms showing:\n",
    "    1. Number of peptides per protein\n",
    "    2. Sequence coverage per protein\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    method_identifiers : Dict\n",
    "        Dictionary with method names as keys, containing 'filtered_df' DataFrames\n",
    "    fasta_human : str\n",
    "        Path to human FASTA file\n",
    "    fasta_yeast : str\n",
    "        Path to yeast FASTA file\n",
    "    fasta_arath : str\n",
    "        Path to A. thaliana FASTA file\n",
    "    output_dir : str, optional\n",
    "        Directory to save output files\n",
    "    figure_size : tuple, default=(16, 10)\n",
    "        Figure size (width, height) in inches\n",
    "    vector_format : str, default='svg'\n",
    "        Vector format for saving ('svg', 'pdf', 'eps')\n",
    "    save_raster : bool, default=True\n",
    "        Whether to save high-resolution raster version\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (summary_df, fig) - Summary statistics DataFrame and matplotlib figure\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import defaultdict\n",
    "    import os\n",
    "    \n",
    "    # Set matplotlib parameters\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'DejaVu Sans',\n",
    "        'font.size': 11,\n",
    "        'axes.titlesize': 14,\n",
    "        'axes.labelsize': 12,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10,\n",
    "        'legend.fontsize': 10,\n",
    "        'figure.titlesize': 16\n",
    "    })\n",
    "    \n",
    "    def parse_fasta(fasta_path):\n",
    "        \"\"\"Parse FASTA file and return dict of {gene_name: (full_key, sequence)}\"\"\"\n",
    "        sequences = {}\n",
    "        current_id = None\n",
    "        current_seq = []\n",
    "        \n",
    "        with open(fasta_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    if current_id:\n",
    "                        # Store with gene name as key\n",
    "                        gene_name = current_id.split('|')[-1].split()[0] if '|' in current_id else current_id.split()[0]\n",
    "                        sequences[gene_name] = (current_id, ''.join(current_seq))\n",
    "                    # Keep full header\n",
    "                    current_id = line[1:]\n",
    "                    current_seq = []\n",
    "                else:\n",
    "                    current_seq.append(line)\n",
    "            \n",
    "            if current_id:\n",
    "                gene_name = current_id.split('|')[-1].split()[0] if '|' in current_id else current_id.split()[0]\n",
    "                sequences[gene_name] = (current_id, ''.join(current_seq))\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def calculate_coverage(peptide_sequence, protein_sequence):\n",
    "        \"\"\"Calculate which positions in protein are covered by peptide\"\"\"\n",
    "        covered_positions = set()\n",
    "        peptide_clean = peptide_sequence.upper()\n",
    "        peptide_clean = ''.join([c for c in peptide_clean if c.isalpha()])\n",
    "        \n",
    "        # Find all occurrences of peptide in protein\n",
    "        start = 0\n",
    "        while True:\n",
    "            pos = protein_sequence.find(peptide_clean, start)\n",
    "            if pos == -1:\n",
    "                break\n",
    "            covered_positions.update(range(pos, pos + len(peptide_clean)))\n",
    "            start = pos + 1\n",
    "        \n",
    "        return covered_positions\n",
    "    \n",
    "    def extract_gene_names(protein_names_str):\n",
    "        \"\"\"Extract gene names from Protein.Names column\"\"\"\n",
    "        # Format: sp|P12345|GENENAME_HUMAN;sp|Q54321|GENENAME2_HUMAN\n",
    "        gene_names = []\n",
    "        if pd.isna(protein_names_str):\n",
    "            return gene_names\n",
    "        \n",
    "        # Split by semicolon for protein groups\n",
    "        parts = str(protein_names_str).split(';')\n",
    "        for part in parts:\n",
    "            # Extract gene name (part after second |)\n",
    "            if '|' in part:\n",
    "                gene_name = part.split('|')[-1].split()[0]\n",
    "                gene_names.append(gene_name)\n",
    "            else:\n",
    "                gene_names.append(part.split()[0])\n",
    "        \n",
    "        return gene_names\n",
    "    \n",
    "    print(\"Loading FASTA files...\")\n",
    "    fasta_files = {\n",
    "        'HUMAN': parse_fasta(fasta_human),\n",
    "        'YEAST': parse_fasta(fasta_yeast),\n",
    "        'ARATH': parse_fasta(fasta_arath)\n",
    "    }\n",
    "    print(f\"  HUMAN: {len(fasta_files['HUMAN'])} proteins\")\n",
    "    print(f\"  YEAST: {len(fasta_files['YEAST'])} proteins\")\n",
    "    print(f\"  ARATH: {len(fasta_files['ARATH'])} proteins\")\n",
    "    \n",
    "    # Analyze each method\n",
    "    results = {}\n",
    "    \n",
    "    for method, identifiers in method_identifiers.items():\n",
    "        print(f\"\\n=== Analyzing {method} ===\")\n",
    "        \n",
    "        if identifiers['filtered_df'].empty:\n",
    "            print(f\"  No data for {method}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        df = identifiers['filtered_df']\n",
    "        \n",
    "        # Get unique peptide-protein pairs\n",
    "        peptide_protein_pairs = df[['Stripped.Sequence', 'Protein.Names', 'Protein.Group']].drop_duplicates()\n",
    "        \n",
    "        # Separate by species\n",
    "        species_data = {\n",
    "            'HUMAN': peptide_protein_pairs[peptide_protein_pairs['Protein.Names'].str.contains('_HUMAN', na=False)],\n",
    "            'YEAST': peptide_protein_pairs[peptide_protein_pairs['Protein.Names'].str.contains('_YEAST', na=False)],\n",
    "            'ARATH': peptide_protein_pairs[peptide_protein_pairs['Protein.Names'].str.contains('_ARATH', na=False)]\n",
    "        }\n",
    "        \n",
    "        method_results = {}\n",
    "        \n",
    "        for species, species_df in species_data.items():\n",
    "            print(f\"\\n  {species}:\")\n",
    "            print(f\"    Total peptide-protein pairs: {len(species_df)}\")\n",
    "            \n",
    "            if species_df.empty:\n",
    "                print(f\"    No data for {species}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Map protein names to gene names and peptides\n",
    "            protein_peptides = defaultdict(set)\n",
    "            protein_to_gene_name = {}\n",
    "            \n",
    "            for _, row in species_df.iterrows():\n",
    "                protein_group = row['Protein.Group']\n",
    "                peptide = row['Stripped.Sequence']\n",
    "                protein_names = row['Protein.Names']\n",
    "                \n",
    "                # Extract gene names from protein names\n",
    "                gene_names = extract_gene_names(protein_names)\n",
    "                \n",
    "                if gene_names:\n",
    "                    # Use first gene name as representative\n",
    "                    gene_name = gene_names[0]\n",
    "                    protein_to_gene_name[protein_group] = gene_name\n",
    "                    protein_peptides[protein_group].add(peptide)\n",
    "                else:\n",
    "                    # Use protein group if no gene name found\n",
    "                    protein_peptides[protein_group].add(peptide)\n",
    "            \n",
    "            # Show some examples of the matching\n",
    "            if len(protein_to_gene_name) > 0:\n",
    "                examples = list(protein_to_gene_name.items())[:3]\n",
    "                print(f\"    Example gene name mappings:\")\n",
    "                for prot, gene in examples:\n",
    "                    in_fasta = \"\" if gene in fasta_files[species] else \"\"\n",
    "                    print(f\"      {prot}  {gene} [{in_fasta}]\")\n",
    "            \n",
    "            print(f\"    Unique proteins: {len(protein_peptides)}\")\n",
    "            print(f\"    Unique peptides: {len(set(species_df['Stripped.Sequence']))}\")\n",
    "            \n",
    "            # Calculate coverage for each protein\n",
    "            peptides_per_protein = []\n",
    "            coverage_per_protein = []\n",
    "            proteins_found = 0\n",
    "            proteins_not_found = 0\n",
    "            \n",
    "            for protein_id, peptides in protein_peptides.items():\n",
    "                peptides_per_protein.append(len(peptides))\n",
    "                \n",
    "                # Get gene name for this protein\n",
    "                gene_name = protein_to_gene_name.get(protein_id)\n",
    "                \n",
    "                if gene_name and gene_name in fasta_files[species]:\n",
    "                    proteins_found += 1\n",
    "                    # Get protein sequence from FASTA\n",
    "                    full_key, protein_seq = fasta_files[species][gene_name]\n",
    "                    \n",
    "                    # Calculate coverage\n",
    "                    covered_positions = set()\n",
    "                    for peptide in peptides:\n",
    "                        covered_positions.update(calculate_coverage(peptide, protein_seq))\n",
    "                    \n",
    "                    coverage_pct = (len(covered_positions) / len(protein_seq)) * 100\n",
    "                    coverage_per_protein.append(coverage_pct)\n",
    "                else:\n",
    "                    # Protein not found in FASTA\n",
    "                    proteins_not_found += 1\n",
    "                    coverage_per_protein.append(np.nan)\n",
    "            \n",
    "            # Remove NaN values for coverage\n",
    "            valid_coverage = [c for c in coverage_per_protein if not np.isnan(c)]\n",
    "            \n",
    "            print(f\"    Proteins found in FASTA: {proteins_found}/{len(protein_peptides)} ({proteins_not_found} not found)\")\n",
    "            print(f\"    Peptides per protein: mean={np.mean(peptides_per_protein):.1f}, \"\n",
    "                  f\"median={np.median(peptides_per_protein):.1f}, \"\n",
    "                  f\"max={np.max(peptides_per_protein)}\")\n",
    "            \n",
    "            if valid_coverage:\n",
    "                print(f\"    Coverage per protein: mean={np.mean(valid_coverage):.1f}%, \"\n",
    "                      f\"median={np.median(valid_coverage):.1f}%\")\n",
    "            else:\n",
    "                print(f\"    Warning: No proteins found in FASTA for coverage calculation\")\n",
    "            \n",
    "            method_results[species] = {\n",
    "                'peptides_per_protein': peptides_per_protein,\n",
    "                'coverage_per_protein': valid_coverage,\n",
    "                'n_proteins': len(protein_peptides),\n",
    "                'n_peptides': len(set(species_df['Stripped.Sequence'])),\n",
    "                'n_proteins_with_seq': len(valid_coverage)\n",
    "            }\n",
    "        \n",
    "        results[method] = method_results\n",
    "    \n",
    "    # Create plots\n",
    "    methods = list(results.keys())\n",
    "    species_list = ['HUMAN', 'YEAST', 'ARATH']\n",
    "    colors = {'HUMAN': '#2E86AB', 'YEAST': '#A23B72', 'ARATH': '#F18F01'}\n",
    "    \n",
    "    # Create figure with 2 rows (peptides per protein, coverage) x 3 columns (species)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=figure_size)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # Row 1: Peptides per protein\n",
    "    for col, species in enumerate(species_list):\n",
    "        ax = axes[0, col]\n",
    "        \n",
    "        for method in methods:\n",
    "            if species in results[method]:\n",
    "                data = results[method][species]['peptides_per_protein']\n",
    "                ax.hist(data, bins=30, alpha=0.6, label=method, \n",
    "                       edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        ax.set_xlabel('Peptides per Protein', fontweight='bold')\n",
    "        ax.set_ylabel('Number of Proteins', fontweight='bold')\n",
    "        ax.set_title(f'{species} - Peptides per Protein', fontweight='bold', fontsize=12)\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Row 2: Sequence coverage\n",
    "    for col, species in enumerate(species_list):\n",
    "        ax = axes[1, col]\n",
    "        \n",
    "        for method in methods:\n",
    "            if species in results[method]:\n",
    "                data = results[method][species]['coverage_per_protein']\n",
    "                if data:  # Only plot if we have coverage data\n",
    "                    ax.hist(data, bins=30, alpha=0.6, label=method,\n",
    "                           edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        ax.set_xlabel('Sequence Coverage (%)', fontweight='bold')\n",
    "        ax.set_ylabel('Number of Proteins', fontweight='bold')\n",
    "        ax.set_title(f'{species} - Sequence Coverage', fontweight='bold', fontsize=12)\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_xlim(0, 100)\n",
    "    \n",
    "    plt.suptitle('Peptide Coverage Analysis by Species and Method', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    summary_data = []\n",
    "    for method in methods:\n",
    "        for species in species_list:\n",
    "            if species in results[method]:\n",
    "                res = results[method][species]\n",
    "                summary_data.append({\n",
    "                    'Method': method,\n",
    "                    'Species': species,\n",
    "                    'N_Proteins': res['n_proteins'],\n",
    "                    'N_Peptides': res['n_peptides'],\n",
    "                    'N_Proteins_With_Sequence': res['n_proteins_with_seq'],\n",
    "                    'Mean_Peptides_Per_Protein': np.mean(res['peptides_per_protein']),\n",
    "                    'Median_Peptides_Per_Protein': np.median(res['peptides_per_protein']),\n",
    "                    'Mean_Coverage_Pct': np.mean(res['coverage_per_protein']) if res['coverage_per_protein'] else np.nan,\n",
    "                    'Median_Coverage_Pct': np.median(res['coverage_per_protein']) if res['coverage_per_protein'] else np.nan\n",
    "                })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Save outputs\n",
    "    saved_files = []\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save summary table\n",
    "        summary_path = os.path.join(output_dir, 'species_coverage_summary.csv')\n",
    "        summary_df.to_csv(summary_path, index=False)\n",
    "        saved_files.append(summary_path)\n",
    "        print(f\"\\nSaved summary table: {summary_path}\")\n",
    "        \n",
    "        # Save detailed data for each method/species\n",
    "        for method in methods:\n",
    "            for species in species_list:\n",
    "                if species in results[method]:\n",
    "                    method_safe = method.replace('/', '_').replace(' ', '_')\n",
    "                    \n",
    "                    # Peptides per protein\n",
    "                    ppp_df = pd.DataFrame({\n",
    "                        'Peptides_Per_Protein': results[method][species]['peptides_per_protein']\n",
    "                    })\n",
    "                    ppp_path = os.path.join(output_dir, \n",
    "                                           f'{method_safe}_{species}_peptides_per_protein.csv')\n",
    "                    ppp_df.to_csv(ppp_path, index=False)\n",
    "                    saved_files.append(ppp_path)\n",
    "                    \n",
    "                    # Coverage per protein\n",
    "                    if results[method][species]['coverage_per_protein']:\n",
    "                        cov_df = pd.DataFrame({\n",
    "                            'Coverage_Percent': results[method][species]['coverage_per_protein']\n",
    "                        })\n",
    "                        cov_path = os.path.join(output_dir,\n",
    "                                               f'{method_safe}_{species}_coverage_per_protein.csv')\n",
    "                        cov_df.to_csv(cov_path, index=False)\n",
    "                        saved_files.append(cov_path)\n",
    "        \n",
    "        # Save vector plot\n",
    "        vector_path = os.path.join(output_dir, f'species_coverage_analysis.{vector_format}')\n",
    "        fig.savefig(vector_path, format=vector_format, bbox_inches='tight',\n",
    "                   facecolor='white', edgecolor='none')\n",
    "        saved_files.append(vector_path)\n",
    "        print(f\"Saved vector plot: {vector_path}\")\n",
    "        \n",
    "        # Save raster if requested\n",
    "        if save_raster:\n",
    "            raster_path = os.path.join(output_dir, 'species_coverage_analysis.png')\n",
    "            fig.savefig(raster_path, dpi=300, bbox_inches='tight',\n",
    "                       facecolor='white', edgecolor='none')\n",
    "            saved_files.append(raster_path)\n",
    "            print(f\"Saved raster plot: {raster_path}\")\n",
    "        \n",
    "        print(f\"\\nTotal files saved: {len(saved_files)}\")\n",
    "    \n",
    "    return summary_df, fig, saved_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f65b2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dict = [\n",
    "        {'instrument': '1per FA', 'method': '', 'file_tags': ['A01','A02','A03'], \n",
    "         'path': r'Z:/Tim_SPEC/fractionation/LTH43_report.parquet'},\n",
    "        {'instrument': '0 mM NaCl', 'method': 'fractionation', 'file_tags': ['A04','A05','A06'], \n",
    "         'path': r'Z:/Tim_SPEC/fractionation/LTH43_report.parquet'},\n",
    "        {'instrument': '50 mM NaCl', 'method': 'fractionation', 'file_tags': ['B04','B05','B06'], \n",
    "         'path': r'Z:/Tim_SPEC/fractionation/LTH43_report.parquet'},\n",
    "        {'instrument': '100 mM NaCl', 'method': 'fractionation', 'file_tags': ['C04','C05','C06'], \n",
    "         'path': r'Z:/Tim_SPEC/fractionation/LTH43_report.parquet'},\n",
    "        {'instrument': '300 mM NaCl', 'method': 'fractionation', 'file_tags': ['D04','D05','D06'], \n",
    "         'path': r'Z:/Tim_SPEC/fractionation/LTH43_report.parquet'},\n",
    "        {'instrument': '1per FA', 'method': 'fractionation', 'file_tags': ['E04','E05','E06'], \n",
    "         'path': r'Z:/Tim_SPEC/fractionation/LTH43_report.parquet'}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98e3a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"Z:/Tim_SPEC/fractionation/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a90dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df, fig, fig_venn, saved_files, method_identifiers = compare_unique_identifiers_across_methods(\n",
    "    experiment_dict,\n",
    "    output_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d1eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_summary, coverage_fig, coverage_files = analyze_species_coverage(\n",
    "    method_identifiers=method_identifiers,\n",
    "    fasta_human='Z:/Tim_SPEC/fractionation/Human.fasta',\n",
    "    fasta_yeast='Z:/Tim_SPEC/fractionation/UP000002311_559292_yeast.fasta',\n",
    "    fasta_arath='Z:/Tim_SPEC/fractionation/UP000006548_3702_arabidopsis.fasta',\n",
    "    output_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd1da495",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_yeast_pept_ss = pd.read_csv(r'Z:/Tim_SPEC/fractionation/output/no_fractionation_YEAST_peptides_per_protein.csv')\n",
    "frac_yeast_pept_frac = pd.read_csv(r'Z:/Tim_SPEC/fractionation/output/fractionation_YEAST_peptides_per_protein.csv')\n",
    "###\n",
    "###\n",
    "frac_human_pept_ss = pd.read_csv(r'Z:/Tim_SPEC/fractionation/output/no_fractionation_HUMAN_peptides_per_protein.csv')\n",
    "frac_human_pept_frac = pd.read_csv(r'Z:/Tim_SPEC/fractionation/output/fractionation_HUMAN_peptides_per_protein.csv')\n",
    "###\n",
    "###\n",
    "frac_ara_pept_ss = pd.read_csv(r'Z:/Tim_SPEC/fractionation/output/no_fractionation_ARATH_peptides_per_protein.csv')\n",
    "frac_ara_pept_frac = pd.read_csv(r'Z:/Tim_SPEC/fractionation/output/fractionation_ARATH_peptides_per_protein.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49f51ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_yeast_cov_ss = pd.read_csv(r'Z:/Tim_SPEC/fractionation/output/no_fractionation_YEAST_coverage_per_protein.csv')\n",
    "frac_yeast_cov_frac = pd.read_csv(r'Z:/Tim_SPEC/fractionation/output/fractionation_YEAST_coverage_per_protein.csv')\n",
    "###\n",
    "###\n",
    "frac_human_cov_ss = pd.read_csv(r'Z:/Tim_SPEC/fractionation/output/no_fractionation_HUMAN_coverage_per_protein.csv')\n",
    "frac_human_cov_frac = pd.read_csv(r'Z:/Tim_SPEC/fractionation/output/fractionation_HUMAN_coverage_per_protein.csv')\n",
    "###\n",
    "###\n",
    "frac_ara_cov_ss = pd.read_csv(r'Z:/Tim_SPEC/fractionation/output/no_fractionation_ARATH_coverage_per_protein.csv')\n",
    "frac_ara_cov_frac = pd.read_csv(r'Z:/Tim_SPEC/fractionation/output/fractionation_ARATH_coverage_per_protein.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=frac_yeast_cov_ss.iloc[:,0].tolist(),\n",
    "    xbins=dict(\n",
    "        start=0,\n",
    "        end=100,\n",
    "        size=3 \n",
    "    ),\n",
    "    marker=dict(\n",
    "        color='#98463f',\n",
    "        line=dict(color='black', width=1)\n",
    "    )\n",
    "))\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=frac_yeast_cov_frac.iloc[:,0].tolist(),\n",
    "    name='Dataset 2', \n",
    "    xbins=dict(start=0, end=100, size=3),\n",
    "    marker=dict(\n",
    "        color='#2b0948',\n",
    "        line=dict(color='black', width=1)\n",
    "    ),\n",
    "    opacity=0.85\n",
    "))\n",
    "fig.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    template='none',\n",
    "    xaxis_title='Coverage (%)',\n",
    "    yaxis_title='Count',\n",
    "    barmode='overlay',\n",
    "    showlegend=False\n",
    ")\n",
    "fig.add_vline(x = np.median(frac_yeast_cov_ss.iloc[:,0].tolist()), line={'dash': 'dash', 'width': 3, 'color': '#98463f'})\n",
    "fig.add_vline(x = np.median(frac_yeast_cov_frac.iloc[:,0].tolist()), line={'dash': 'dash', 'width': 3, 'color': '#000814'})\n",
    "#fig.write_image(r'D:\\Projects\\SPEC\\figs_raw\\Figure_2e1.pdf', width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a57ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=frac_human_cov_ss.iloc[:,0].tolist(),\n",
    "    xbins=dict(\n",
    "        start=0,\n",
    "        end=100,\n",
    "        size=3 \n",
    "    ),\n",
    "    marker=dict(\n",
    "        color='#98463f',\n",
    "        line=dict(color='black', width=1)\n",
    "    )\n",
    "))\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=frac_human_cov_frac.iloc[:,0].tolist(),\n",
    "    name='Dataset 2', \n",
    "    xbins=dict(start=0, end=100, size=3),\n",
    "    marker=dict(\n",
    "        color='#2b0948',\n",
    "        line=dict(color='black', width=1)\n",
    "    ),\n",
    "    opacity=0.85\n",
    "))\n",
    "fig.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    template='none',\n",
    "    xaxis_title='Coverage (%)',\n",
    "    yaxis_title='Count',\n",
    "    barmode='overlay',\n",
    "    showlegend=False\n",
    ")\n",
    "fig.add_vline(x = np.median(frac_human_cov_ss.iloc[:,0].tolist()), line={'dash': 'dash', 'width': 3, 'color': '#98463f'})\n",
    "fig.add_vline(x = np.median(frac_human_cov_frac.iloc[:,0].tolist()), line={'dash': 'dash', 'width': 3, 'color': '#000814'})\n",
    "\n",
    "#fig.write_image(r'D:\\Projects\\SPEC\\figs_raw\\Figure_2e2.pdf', width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02461081",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=frac_ara_cov_ss.iloc[:,0].tolist(),\n",
    "    xbins=dict(\n",
    "        start=0,\n",
    "        end=100,\n",
    "        size=3 \n",
    "    ),\n",
    "    marker=dict(\n",
    "        color='#98463f',\n",
    "        line=dict(color='black', width=1)\n",
    "    )\n",
    "))\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=frac_ara_cov_frac.iloc[:,0].tolist(),\n",
    "    name='Dataset 2', \n",
    "    xbins=dict(start=0, end=100, size=3),\n",
    "    marker=dict(\n",
    "        color='#2b0948',\n",
    "        line=dict(color='black', width=1)\n",
    "    ),\n",
    "    opacity=0.85\n",
    "))\n",
    "fig.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    template='none',\n",
    "    xaxis_title='Coverage (%)',\n",
    "    yaxis_title='Count',\n",
    "    barmode='overlay',\n",
    "    showlegend=False\n",
    ")\n",
    "fig.add_vline(x = np.median(frac_ara_cov_ss.iloc[:,0].tolist()), line={'dash': 'dash', 'width': 3, 'color': '#98463f'})\n",
    "fig.add_vline(x = np.median(frac_ara_cov_frac.iloc[:,0].tolist()), line={'dash': 'dash', 'width': 3, 'color': '#000814'})\n",
    "#fig.write_image(r'D:\\Projects\\SPEC\\figs_raw\\Figure_2e3.pdf', width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a79044",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(y = frac_yeast_pept_ss.iloc[:,0].tolist(), marker_color = '#98463f'))\n",
    "fig.add_trace(go.Box(y = frac_yeast_pept_frac.iloc[:,0].tolist(), marker_color = '#2b0948'))\n",
    "fig.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    template='none',\n",
    "    showlegend=False\n",
    ")\n",
    "fig.add_hline(y = np.nanmedian(frac_yeast_pept_ss.iloc[:,0].tolist()), line={'dash': 'dash', 'width': 3, 'color': '#98463f'})\n",
    "fig.add_hline(y = np.nanmedian(frac_yeast_pept_frac.iloc[:,0].tolist()), line={'dash': 'dash', 'width': 3, 'color': '#000814'})\n",
    "#fig.write_image(r'D:\\Projects\\SPEC\\figs_raw\\Figure_2e4.pdf', width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40312e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(y = frac_human_pept_ss.iloc[:,0].tolist(), marker_color = '#98463f'))\n",
    "fig.add_trace(go.Box(y = frac_human_pept_frac.iloc[:,0].tolist(), marker_color = '#2b0948'))\n",
    "fig.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    template='none',\n",
    "    showlegend=False\n",
    ")\n",
    "fig.add_hline(y = np.nanmedian(frac_human_pept_ss.iloc[:,0].tolist()), line={'dash': 'dash', 'width': 3, 'color': '#98463f'})\n",
    "fig.add_hline(y = np.nanmedian(frac_human_pept_frac.iloc[:,0].tolist()), line={'dash': 'dash', 'width': 3, 'color': '#000814'})\n",
    "#fig.write_image(r'D:\\Projects\\SPEC\\figs_raw\\Figure_2e5.pdf', width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(y = frac_human_pept_ss.iloc[:,0].tolist(), marker_color = '#98463f'))\n",
    "fig.add_trace(go.Box(y = frac_human_pept_frac.iloc[:,0].tolist(), marker_color = '#2b0948'))\n",
    "fig.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    template='none',\n",
    "    showlegend=False\n",
    ")\n",
    "fig.add_hline(y = np.nanmedian(frac_human_pept_ss.iloc[:,0].tolist()), line={'dash': 'dash', 'width': 3, 'color': '#98463f'})\n",
    "fig.add_hline(y = np.nanmedian(frac_human_pept_frac.iloc[:,0].tolist()), line={'dash': 'dash', 'width': 3, 'color': '#000814'})\n",
    "#fig.write_image(r'D:\\Projects\\SPEC\\figs_raw\\Figure_2e5.pdf', width = 600, height = 600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
